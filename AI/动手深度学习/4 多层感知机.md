![[Pasted image 20230516170750.png|400]]

### 4.3 多层感知机简洁实现

```python
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(), nn.Linear(256,10))

def init_weight(m):
if type(m) == nn.Linear:
	nn.init.normal_(m.weight, std=0.01)

net.apply(init_weight);

batch_size, lr, num_epochs = 256, 0.1, 10
loss = nn.CrossEntropyLoss(reduction='none')
trainer = torch.optim.SGD(net.parameters(), lr=lr)

d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

### 4.4 过拟合与欠拟合

### 4.5 权重衰减 Weight Decay

使用$L_2$对权重正则化。

```python
trainer = torch.optim.SGD([
	{"params":net[0].weight, 'weight_decay' : wd},
	{"params":net[0].bias}], lr=lr
)
```

### 4.6 暂退 Dropout

微小的噪音不应该造成影响输出的结果。以此为灵感，在向前传播的每一层注入噪音。在神经网络中，我们在训练时随机丢弃一些节点，而在推理阶段保留所有的节点。在Pytorch中，我们只需要在每个全连接层之后添加一个Dropout层。

```python
nn.Sequential(nn.Flatten(),
			  nn.Linear(784, 256),
			  nn.ReLU(),
			  nn.Dropout(dropout1),
			  nn.Linear(256, 10))
```


### 4.7 正、反向传播计算图

反向传播重复利用向前传播的中间值避免重复计算，所以训练过程（反向传播）时往往需要更多的显存。

### 4.8 数值稳定性与模型初始化

不恰当的初始化可能会带来梯度消失和梯度爆炸（无法收敛）。sigmoid函数经常导致梯度消失。