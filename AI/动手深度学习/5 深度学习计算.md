### 4.1 层和块

* 块是多个层组成的组件。
* `nn.Sequential`定义了一个由Module构成的有序列表。Linear类是Module的一个子类。

```python
// 自定义块
class MLP(nn.Module):
	# 初始化
	def __init__(self):
		super().__init__()
		self.hidden = nn.Linear(20, 256)
		self.out = nn.Linear(256, 10)

	# 定义向前传播
	def forward(self, X):
		return self.out(F.relu(self.hidden(X)))
```

### 4.2 参数管理

```python
# 参数访问
print(net[2].state_dict)
net[2].weight.grad == None

print(*[(name, param.shape) for name, param in net[0].named_parameters()])
print(*[(name, param.shape) for name, param in net.named_parameters()])

# 参数初始化
# 内置初始化
def init_normal(m):
	if type(m) == nn.Linear:
		nn.init.normal_(m.weight, mean=0, std=0.01)
		nn.init.zeros_(m.bias)
net.apply(init_normal)
```

### 4.3 延后初始化

在定义稠密层的时候可以不指定输入的大小。

```python
# tensorflow & mxnet
def get_net():
	net = nn.Sequential()
	net.add(nn.Dense(256, activation='relu'))

# pytorch
net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.Linear(256,10))
```

### 4.4 自定义层

```python
class MyLinear(nn.Module):
	def __init__(self, in_units, units):
		super().__init__()
		self.weight = nn.Parameter(torch.randn(in_units, units))
		self.bias = nn.Parameter(torch.randn(units,))
	def forward(self, X):
		linear = torch.matmul(X, selfr.weight.data) + self.bias.data
		return F.relu(linear)
```
