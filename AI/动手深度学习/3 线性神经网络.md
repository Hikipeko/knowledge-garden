* 对计算进行矢量化，避免for循环
* 使用DataLoader充分利用GPU并行运算的优势
* 在每个迭代周期（epoch）中，使用Data Iterator遍历整个数据集

### Linear Regression

```python
from torch import nn

net = nn.Sequential(nn.Linear(2, 1))
net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)

loss = nn.MSELoss()
trainer = torch.optim.SGD(net.parameters(), lr=0.03)

num_epochs = 3
for epoch in range(num_epochs):
	for X, y in data_iter:
		l = loss(net(X), y) # forward prop
		trainer.zero_grad()
		l.backward() # backward prop
		trainer.step() # update
	l = loss(net(features), albels)
	print(f"epoch {epch + 1}, loss {1:f}")
```

### softmax Regression

```python
def train_epoch_ch3(net, train_iter, loss, updater):
	# total_loss, total_accuracy, num_sample
	metric = Accumulator(3)
	if isinstance(updater, gluon.Trainer):
		updater = updater.step
	for X, y in train_iter:
		with autograd.record():
			y_hat = net(X)
			l = loss(y_hat, y)
		l.backward()
		updater(X.shape[0])
		metric.add(float(l.sum()), accuracy(y_hat, y), y.size)
	return metric[0] / metric[2], metric[1] / metric[2]

net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))
net.apply(init_weights)

loss = nn.CrossEntrophyLoss(reduction='none')
trainer = torch.optim.SGD(net.parameters(), lr=0.1)
num_epochs = 10
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```
